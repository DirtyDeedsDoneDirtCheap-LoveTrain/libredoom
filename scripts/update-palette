#!/usr/bin/env python3
# SPDX-License-Identifier: BSD-3-Clause
#
# update-palette - Update the palettes of all PNG graphics
#
# All of the PNGs in Freedoom are paletted, and the palettes of each PNG match
# the colours in the PLAYPAL lump. If a user wants to make changes to the
# palette, they would have to update the palette in all of Freedoom's graphics
# for consistency.
#
# This script takes a new PLAYPAL as an argument, compares the old and new
# palettes, and modifies every paletted PNG file in the repo so that the new
# colours are used.
#
# Optional dependencies:
# - pypng: https://gitlab.com/drj11/pypng (retrieved Sept 10 2023)

import argparse
from collections import namedtuple
from functools import reduce
from itertools import chain, starmap, tee, zip_longest
from math import floor, ceil
import struct
import os
from os.path import dirname, relpath, realpath, join, normpath
# import png
from sys import argv
import zlib

PngChunk = namedtuple("PNGChunk", "type data")

PNG_SIGNATURE = b"\x89PNG\r\n\x1a\n"

# Parse the command line arguments, and return a dict with the arguments
def parse_args():
    parser = argparse.ArgumentParser(
        "update-palette",
        description="This script takes a new palette, compares the new "
        "palette with the old one, and scans and updates images in the repo "
        "which use the colours that were replaced in the new palette. This "
        "script only supports indexed-colour PNGs. You may need to run 'make "
        "fix-deutex-pngs' before running this script in order to palettize "
        "any true-colour PNGs in the repo."
    )
    parser.add_argument("palette", help="The new palette to use")
    # This is a potential vulnerability, and besides, it doesn't make
    # sense to update the palette for images which aren't in the repo.
    # parser.add_argument(
    #     "--dir", "-d", help=(
    #         "The directory to recursively process. "
    #         "Defaults to repository root directory."),
    #     default=normpath(join(dirname(realpath(argv[0])), "..")))
    parser.add_argument(
        "-d", "--dry-run",
        help="Do not modify any PNGs, just show which ones would be modified",
        action='store_true')
    args = parser.parse_args()
    return args


# https://docs.python.org/3.8/library/itertools.html#itertools-recipes
def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx"
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)


# https://docs.python.org/3.8/library/itertools.html#itertools-recipes
def flatten(list_of_lists):
    "Flatten one level of nesting"
    return chain.from_iterable(list_of_lists)


# Compare the old palette and the new palette, and return a dict with the
# differences.
def compare_palettes(directory, new_palette):
    old_palette = join(directory, "lumps", "playpal", "playpal-base.lmp")

    # The "new" palette is the old palette?
    old_pal_full_path = relpath(normpath(old_palette), directory)
    new_pal_full_path = relpath(normpath(new_palette), directory)
    if old_pal_full_path == new_pal_full_path:
        raise ValueError("You're trying to replace the old palette with "
                         "itself! Try another palette.")

    # Read both palettes into a more usable format
    with open(old_palette, "rb") as handle:
        old_palette = handle.read(768)
        if len(old_palette) < 768:
            raise ValueError("Old palette is too short!")
        old_palette = list(grouper(old_palette[:768], 3))

    with open(new_palette, "rb") as handle:
        new_palette = handle.read(768)
        if len(new_palette) < 768:
            raise ValueError("New palette is too short!")
        new_palette = list(grouper(new_palette[:768], 3))

    # Given a colour palette and a dict, return a dict with the indexes of
    # each colour. This function is meant to be used with functools.reduce.
    def get_duplicate_colours(value, index_colour):
        index, colour = index_colour
        value.setdefault(colour, []).append(index)
        return value

    # Scan the old palette for duplicate colours
    old_palette_duplicates = reduce(
        get_duplicate_colours, enumerate(old_palette), {})
    # Eliminate unique colours
    old_palette_duplicates = dict(filter(
        # kv[1] is the value, and it's unique if it's length is 1
        lambda kv: None if len(kv[1]) == 1 else kv,
        old_palette_duplicates.items()
    ))

    # Map old indices to new colours for now.
    old_to_new = {}
    for index, zipper in enumerate(zip(old_palette, new_palette)):
        old_colour, new_colour = zipper
        if old_colour != new_colour:
            old_to_new[index] = new_colour

    # Does the new palette replace ALL instances of a duplicate colour in the
    # old palette? If so, map the old colour to the first replacement in the
    # new palette, but if not, leave the old colour unchanged.
    replaced = {}
    for colour, indices in old_palette_duplicates.items():
        if all(map(lambda i: i in old_to_new.keys(), indices)):
            replaced[colour] = old_to_new[min(indices)]
        else:
            replaced[colour] = None

    # Replace the keys in old_to_new, which are indices, with the old palette
    # colours they correspond to. This way, we have a colour-to-colour dict.
    old_to_new = dict(filter(lambda kv: kv[1], map(
        lambda iv: (old_palette[iv[0]],
                    replaced.get(old_palette[iv[0]], iv[1])),
        old_to_new.items()
    )))
    return old_to_new


# "Stolen" from the map-color-index script
# Process a directory recursively for PNG files.
def process_dir(colour_map, dry_run, directory, palette):
    pngs_changed_count = 0
    pngs_examined_count = 0

    for dirpath, dirnames, filenames in os.walk(directory):
        for png_base in filenames:
            if not png_base.lower().endswith(".png"):
                continue
            png_path = os.path.join(dirpath, png_base)
            pngs_examined_count += 1
            if process_png(colour_map, png_path, dry_run, directory):
                pngs_changed_count += 1


# https://docs.python.org/3.8/library/itertools.html#itertools-recipes
def pairwise(iterable):
    "s -> (s0,s1), (s1,s2), (s2, s3), ..."
    a, b = tee(iterable)
    next(b, None)
    return zip(a, b)


def process_idat(
    data, width, bit_depth, compression, filtering, interlacing
    ):

    def mask_bits(bit_depth):
        base_bit_mask = reduce(lambda a, b: a | (1 << b), range(bit_depth), 0)
        bit_mask = base_bit_mask
        shift = 0
        while bit_mask:
            yield bit_mask, shift
            bit_mask = (bit_mask << bit_depth) & 0xFF
            shift += bit_depth

    def paeth_predictor(a_byte, b_byte, c_byte):
        # Paeth predictor of x:
        paeth = a_byte + b_byte - c_byte
        pa = abs(paeth - a_byte)
        pb = abs(paeth - b_byte)
        pc = abs(paeth - c_byte)
        if pa <= pb and pa <= pc:
            predictor = a_byte
        elif pb <= pc:
            predictor = b_byte
        else:
            predictor = c_byte
        return predictor

    def defilter(data, width, bit_depth):
        stride = ceil(width / 8 * bit_depth)
        filtered_stride = stride + 1
        bgn_end = pairwise(range(0, len(data), filtered_stride))
        lines = []
        for bgn, end in bgn_end:
            prev_bgn = bgn - filtered_stride + 1
            filtered = data[bgn:end]
            filter_type = filtered[0]
            scanline = tuple(chain((0,), filtered[1:]))
            if prev_bgn >= 0:
                # Reconstructed scanlines in lines list all start with a zero
                prev_scanline = tuple(lines[-1])
            else:
                prev_scanline = tuple([0 for _ in range(filtered_stride)])
            # c b
            # a x
            reconstructed = bytearray(1)
            for byte_index in range(1, len(scanline)):
                unfilters = {  # Filter type to reconstruction function
                    # No filter
                    0: lambda fx, ra, rb, rc: fx % 256,
                    # Sub
                    1: lambda fx, ra, rb, rc: (fx + ra) % 256,
                    # Up
                    2: lambda fx, ra, rb, rc: (fx + rb) % 256,
                    # Average
                    3: lambda fx, ra, rb, rc: \
                        (fx + floor((ra + rb) / 2)) % 256,
                    # Paeth
                    4: lambda fx, ra, rb, rc: \
                        (fx + paeth_predictor(ra, rb, rc)) % 256
                }
                x_byte = scanline[byte_index]
                a_byte = reconstructed[byte_index - 1]
                b_byte = prev_scanline[byte_index]
                c_byte = prev_scanline[byte_index - 1]
                unfilter = unfilters[filter_type]
                reconstructed.append(unfilter(x_byte, a_byte, b_byte, c_byte))
            lines.append(reconstructed)
        # Remove zeros from scanlines in lines list
        return bytes(flatten(map(lambda l: l[1:], lines)))

    # Expand the packed bytes into colour or index samples
    def expand_bytes(data, bit_depth):
        if bit_depth not in {1,2,4,8}:
            return None

        # No expansion necessary
        if bit_depth == 8:
            return data

        return bytes(flatten(map(
            lambda b: bytes(starmap(
                lambda mask, shift: (b & mask) >> shift,
                mask_bits(bit_depth))),
            data)))

    if compression == 0:
        data = zlib.decompress(data)
    if filtering == 0:
        data = defilter(data, width, bit_depth)
    if interlacing != 0:
        print("WARNING! This image is interlaced! De-interlacing is not"
              "implemented yet!")
    data = expand_bytes(data, bit_depth)
    return data


# Process a PNG file in place.
def process_png(colour_map, png_path, dry, directory):

    # Read a chunk from the PNG file
    def read_png_chunk(png_file):
        chunk_len = png_file.read(4)
        if chunk_len == b"": return None  # End of file
        chunk_len = struct.unpack("!I", chunk_len)[0]
        chunk_type = png_file.read(4)
        chunk_data = png_file.read(chunk_len)
        chunk_crc = png_file.read(4)
        chunk_crc = struct.unpack("!I", chunk_crc)[0]
        # crc = zlib.crc32(chunk_type)
        # crc = zlib.crc32(chunk_data, crc)
        # if crc != chunk_crc:
        #     return None
        return PngChunk(chunk_type, chunk_data)

    # Read the PNG file
    chunks = []
    with open(png_path, "rb") as handle:
        if handle.read(8) != PNG_SIGNATURE:
            print("{0} is not a valid PNG file!".format(png_path))
            return False
        while chunk := read_png_chunk(handle):
            chunks.append(chunk)

    # Change the old colours to the new colours
    modified_palette_colours = set()
    def maybe_modify_plte(plte_data):
        nonlocal colour_map
        nonlocal modified_palette_colours
        modified = False
        colours = list(grouper(plte_data, 3))
        for index, colour in enumerate(colours):
            if colour in colour_map:
                modified = True
                modified_palette_colours.add(index)
                colours[index] = colour_map[colour]
        colours = b"".join(map(bytes, colours))
        return modified, colours

    def is_colour_changed(colour_index):
        nonlocal modified_palette_colours
        return colour_index in modified_palette_colours

    # Modify the PLTE chunk, if necessary, and check if the PLTE modifications
    # affect the IDAT chunk.
    plte_modified = False
    idat_modified = False
    is_paletted = False
    all_idat = []
    for index, chunk in enumerate(chunks):
        if chunk.type == b"IHDR":
            (width, height, bit_depth, colour_type,
             compression, filtering, interlacing) = struct.unpack_from(
                "!IIBBBBB", chunk.data
            )
            is_paletted = colour_type == 3 and (
                bit_depth == 1 or bit_depth == 2 or
                bit_depth == 4 or bit_depth == 8
            )
        elif is_paletted and chunk.type == b"PLTE":
            chunk_name = chunk.type
            plte_modified, chunk_data = maybe_modify_plte(chunk.data)
            chunks[index] = PngChunk(chunk_name, chunk_data)
        elif chunk.type == b"IDAT":
            all_idat.append(chunk.data)
    if plte_modified:
        all_idat = bytes(flatten(all_idat))
        values = process_idat(all_idat, width, bit_depth,
                              compression, filtering, interlacing)
        # png_reader = png.Reader(filename=png_path)
        # width, height, values, info = png_reader.read_flat()
        idat_modified = any(map(is_colour_changed, values))
        del values
    del all_idat

    # Write the modified PNG file
    if idat_modified:
        print("{} was changed".format(relpath(png_path, start=directory)))
        if not dry:
            with open(png_path, "wb") as handle:
                handle.write(PNG_SIGNATURE)
                for chunk in chunks:
                    chunk_crc = zlib.crc32(chunk.type)
                    chunk_crc = zlib.crc32(chunk.data, chunk_crc)
                    chunk_crc = struct.pack("!I", chunk_crc)
                    chunk_len = struct.pack("!I", len(chunk.data))
                    handle.write(chunk_len)
                    handle.write(chunk.type)
                    handle.write(chunk.data)
                    handle.write(chunk_crc)

    return idat_modified


if __name__ == "__main__":
    args = parse_args()
    directory = normpath(join(dirname(realpath(argv[0])), ".."))
    comparison = compare_palettes(directory, args.palette)
    process_dir(comparison, directory=directory, **vars(args))
    # Replace old playpal-base.lmp
    if not args.dry_run:
        playpal_base_path = (
            join(directory, "lumps", "playpal", "playpal-base.lmp"))
        # The old/new palette being the same is checked in compare_palettes
        with  open(args.palette, "rb") as new_palfile, \
              open(playpal_base_path, "wb") as old_palfile:
            # Only copy the first 768 bytes of the new palette to playpal-base
            new_pal = new_palfile.read(768)
            # Palette length is checked in compare_palettes
            old_palfile.write(new_pal)
